"""
Comparison demo: Regex-based vs spaCy-based Japanese text processing.
Shows the concrete improvements you get with spaCy integration.
"""

import re
from japanese_processor import get_japanese_processor


def regex_split_sentences(text: str) -> list:
    """Original regex-based sentence splitting."""
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', text)
    result = []
    current = ""

    for part in sentences:
        current += part
        if part in 'ã€‚ï¼ï¼Ÿ':
            if current.strip():
                result.append(current.strip())
            current = ""

    if current.strip():
        result.append(current.strip())

    return result


def spacy_split_sentences(text: str) -> list:
    """spaCy-based sentence splitting."""
    processor = get_japanese_processor(use_spacy=True)
    return processor.split_sentences(text)


def demo_sentence_splitting():
    """Compare sentence splitting methods."""
    print("\n" + "="*70)
    print("DEMO 1: Sentence Splitting Comparison")
    print("="*70)

    # Test case 1: Simple sentences
    test1 = "ç§ã¯å­¦ç”Ÿã§ã™ã€‚æ±äº¬ã«ä½ã‚“ã§ã„ã¾ã™ã€‚æ¯æ—¥å‹‰å¼·ã—ã¾ã™ã€‚"

    print("\nğŸ“ Test 1: Simple sentences")
    print(f"Input: {test1}\n")

    print("Regex-based:")
    regex_result = regex_split_sentences(test1)
    for i, sent in enumerate(regex_result, 1):
        print(f"  {i}. {sent}")

    print("\nspaCy-based:")
    spacy_result = spacy_split_sentences(test1)
    for i, sent in enumerate(spacy_result, 1):
        print(f"  {i}. {sent}")

    # Test case 2: Complex sentences with embedded clauses
    test2 = "ç§ãŒæ˜¨æ—¥è²·ã£ãŸæœ¬ã¯ã€ã¨ã¦ã‚‚é¢ç™½ã„ã§ã™ã€‚å‹é”ã«å‹§ã‚ãŸã„ã¨æ€ã„ã¾ã™ã€‚"

    print("\nğŸ“ Test 2: Complex sentences with embedded clauses")
    print(f"Input: {test2}\n")

    print("Regex-based:")
    regex_result = regex_split_sentences(test2)
    for i, sent in enumerate(regex_result, 1):
        print(f"  {i}. {sent}")

    print("\nspaCy-based:")
    spacy_result = spacy_split_sentences(test2)
    for i, sent in enumerate(spacy_result, 1):
        print(f"  {i}. {sent}")


def demo_tokenization():
    """Compare tokenization methods."""
    print("\n" + "="*70)
    print("DEMO 2: Tokenization Comparison")
    print("="*70)

    text = "ç§ã¯æ—¥æœ¬èªã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚"

    print(f"\nğŸ“ Input: {text}\n")

    # Simple split (what regex would do)
    print("Simple character split:")
    chars = list(text)
    print(f"  {chars}")
    print(f"  Count: {len(chars)} characters")

    # spaCy tokenization
    processor = get_japanese_processor(use_spacy=True)
    tokens = processor.tokenize(text)

    print("\nspaCy tokenization (linguistic units):")
    print(f"  {tokens}")
    print(f"  Count: {len(tokens)} tokens")

    print("\nğŸ’¡ Notice: spaCy breaks å‹‰å¼·ã—ã¦ã„ã¾ã™ into meaningful units:")
    print("  å‹‰å¼· (study) + ã— (verb stem) + ã¦ (te-form) + ã„ (auxiliary) + ã¾ã™ (polite)")


def demo_linguistic_analysis():
    """Show linguistic features from spaCy."""
    print("\n" + "="*70)
    print("DEMO 3: Linguistic Analysis (spaCy-only feature)")
    print("="*70)

    text = """
    ç§ã¯æ±äº¬å¤§å­¦ã®å­¦ç”Ÿã§ã™ã€‚
    æ¯æ—¥ã€å›³æ›¸é¤¨ã§æ—¥æœ¬èªã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚
    æ¥å¹´ã€JLPTã®N2è©¦é¨“ã‚’å—ã‘ã‚‹äºˆå®šã§ã™ã€‚
    """

    processor = get_japanese_processor(use_spacy=True)
    features = processor.extract_linguistic_features(text)

    print(f"\nğŸ“ Input:")
    print(text)

    print("\nğŸ“Š Analysis Results:")
    print(f"  Sentences: {features['sentence_count']}")
    print(f"  Tokens: {features['token_count']}")

    print(f"\n  Character Distribution:")
    for char_type, count in features['character_counts'].items():
        print(f"    {char_type}: {count}")

    if 'pos_counts' in features:
        print(f"\n  Part-of-Speech Distribution:")
        for pos, count in sorted(features['pos_counts'].items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"    {pos}: {count}")

    if 'entities' in features:
        print(f"\n  Named Entities Found:")
        for entity in features['entities']:
            print(f"    â€¢ {entity['text']} ({entity['label']})")


def demo_chunking():
    """Compare chunking strategies."""
    print("\n" + "="*70)
    print("DEMO 4: Smart Chunking Boundaries")
    print("="*70)

    text = """
    æ—¥æœ¬èªã®åŠ©è©ã¯é›£ã—ã„ã§ã™ã€‚ã€Œã¯ã€ã¨ã€ŒãŒã€ã®é•ã„ã‚’ç†è§£ã™ã‚‹ã®ã¯å¤§å¤‰ã§ã™ã€‚
    ã—ã‹ã—ã€ãŸãã•ã‚“ç·´ç¿’ã™ã‚Œã°ã€ã ã‚“ã ã‚“åˆ†ã‹ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
    æ¯æ—¥å°‘ã—ãšã¤å‹‰å¼·ã™ã‚‹ã“ã¨ãŒå¤§åˆ‡ã§ã™ã€‚è«¦ã‚ãšã«é ‘å¼µã‚Šã¾ã—ã‚‡ã†ã€‚
    """

    processor = get_japanese_processor(use_spacy=True)

    print(f"\nğŸ“ Input text ({len(text)} chars):")
    print(text)

    # Get smart boundaries
    boundaries = processor.smart_chunk_boundaries(text, target_size=60, max_size=100)

    print(f"\nğŸ“Š spaCy Smart Chunking (target: 60 chars, max: 100 chars):")
    print(f"Created {len(boundaries)} chunks at natural sentence boundaries:\n")

    for i, (start, end) in enumerate(boundaries, 1):
        chunk = text[start:end].strip()
        print(f"Chunk {i} ({len(chunk)} chars):")
        print(f"  {chunk}\n")

    print("ğŸ’¡ Notice: Each chunk ends at a sentence boundary (ã€‚)")
    print("   This preserves semantic coherence for better embeddings!")


def demo_benefits_for_rag():
    """Show benefits for RAG/LanceDB."""
    print("\n" + "="*70)
    print("DEMO 5: Benefits for RAG & Vector Search")
    print("="*70)

    print("\nğŸ¯ With spaCy integration, your LanceDB chunks will have:")

    print("\n1. **Better Context Preservation**")
    print("   âŒ Regex: 'ã“ã‚Œã¯æœ¬ã§ã™ã€‚ç§ã¯å­¦' (cuts mid-sentence)")
    print("   âœ… spaCy: 'ã“ã‚Œã¯æœ¬ã§ã™ã€‚' (complete sentence)")

    print("\n2. **Smarter Metadata for Filtering**")
    print("   â€¢ Named entities: Filter by location, person names")
    print("   â€¢ POS distribution: Find example sentences vs explanations")
    print("   â€¢ Complexity metrics: Match to user's JLPT level")

    print("\n3. **Improved Embeddings**")
    print("   â€¢ Sentence-level chunks = better semantic similarity")
    print("   â€¢ No broken context = more accurate retrieval")

    print("\n4. **Rich Metadata Example**")
    print("   {")
    print('     "content": "æ±äº¬ã¯æ—¥æœ¬ã®é¦–éƒ½ã§ã™ã€‚",')
    print('     "has_japanese": true,')
    print('     "jlpt_level": "N5",')
    print('     "content_type": "grammar",')
    print('     "pos_distribution": {"NOUN": 3, "VERB": 1, "PARTICLE": 2},')
    print('     "entities": [{"text": "æ±äº¬", "label": "GPE"}, {"text": "æ—¥æœ¬", "label": "GPE"}],')
    print('     "sentence_count": 1,')
    print('     "token_count": 11')
    print("   }")


def main():
    """Run all demos."""
    print("\n" + "="*70)
    print("ğŸ¯ YUKIO - Japanese Processing: Regex vs spaCy Comparison")
    print("="*70)

    demo_sentence_splitting()
    demo_tokenization()
    demo_linguistic_analysis()
    demo_chunking()
    demo_benefits_for_rag()

    print("\n" + "="*70)
    print("âœ… Demo Complete!")
    print("="*70)
    print("\nTo integrate spaCy into your pipeline, see INTEGRATION_GUIDE.md")
    print()


if __name__ == "__main__":
    main()
